{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "import random\n",
    "import scipy as sp\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch import Tensor\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import SGD, Adam, RMSprop\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.utils.data.sampler import SubsetRandomSampler,WeightedRandomSampler\n",
    "from sklearn.metrics import r2_score\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "manualSeed = 1 # random.randint(1, 10000) # Set the desired seed to reproduce the results\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath,device):\n",
    "    '''\n",
    "        Loading an instance of EikoNet from a saved .pt\n",
    "    \n",
    "    \n",
    "        INPUTS:\n",
    "            filepath - path to .pt file\n",
    "            \n",
    "        OUTPUTS:\n",
    "            model - Eikonet Model\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    checkpoint = torch.load(filepath)\n",
    "    # Defining Neural Network\n",
    "    net = Reg_model().to(device)\n",
    "    net.apply(init_weights)\n",
    "    net.float()\n",
    "    net.to(device)\n",
    "    optimizer  = torch.optim.Adam(net.parameters())\n",
    "    scheduler = []\n",
    "    model = Model(net, optimizer,scheduler, model_path=filepath,model_name='')\n",
    "    model.total_train_loss = checkpoint['train_loss']\n",
    "    model.total_val_loss   = checkpoint['val_loss']\n",
    "\n",
    "    model.network.load_state_dict(checkpoint['model_state_dict'])\n",
    "    for parameter in model.network.parameters():\n",
    "        parameter.requires_grad = False\n",
    "\n",
    "    model.network.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reg_model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Defining the neural network\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        m = 1\n",
    "        # Layers \n",
    "        self.fc0  = Linear(299,299*m)\n",
    "        self.fc1  = Linear(299*m,299*m)\n",
    "\n",
    "        # resnet - block 1\n",
    "        self.rn1_fc1  = Linear(299*m,299*m)\n",
    "        self.rn1_fc2  = Linear(299*m,299*m)\n",
    "        self.rn1_fc3  = Linear(299*m,299*m)\n",
    "\n",
    "        # resnet - block 2 \n",
    "        self.rn2_fc1  = Linear(299*m,299*m)\n",
    "        self.rn2_fc2  = Linear(299*m,299*m)\n",
    "        self.rn2_fc3  = Linear(299*m,299*m)\n",
    "\n",
    "        # resnet - block 3\n",
    "        self.rn3_fc1  = Linear(299*m,299*m)\n",
    "        self.rn3_fc2  = Linear(299*m,299*m)\n",
    "        self.rn3_fc3  = Linear(299*m,299*m)\n",
    "\n",
    "\n",
    "        # Output structure\n",
    "        self.fc8  = Linear(299*m,299)\n",
    "        self.fc9  = Linear(299,125)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x   = self.relu(self.fc0(x))\n",
    "        x   = self.relu(self.fc1(x))\n",
    "\n",
    "        # Resnet - Block 1\n",
    "        x0  = x\n",
    "        x   = self.relu(self.rn1_fc1(x))\n",
    "        x   = self.relu(self.rn1_fc3(x) + self.rn1_fc2(x0))\n",
    "\n",
    "\n",
    "        # Resnet - Block 2\n",
    "        x0  = x\n",
    "        x   = self.relu(self.rn2_fc1(x))\n",
    "        x   = self.relu(self.rn2_fc3(x)+self.rn2_fc2(x0))\n",
    "\n",
    "#         # Resnet - Block 3\n",
    "        x0  = x\n",
    "        x   = self.relu(self.rn3_fc1(x))\n",
    "        x   = self.relu(self.rn3_fc3(x)+self.rn3_fc2(x0))\n",
    "\n",
    "\n",
    "\n",
    "        # Joining two blocks\n",
    "        x     = self.relu(self.fc8(x))\n",
    "        x   = self.fc9(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == torch.nn.Linear:\n",
    "        stdv = (1. / math.sqrt(m.weight.size(1))/1.)*2\n",
    "        m.weight.data.uniform_(-stdv,stdv)\n",
    "        m.bias.data.uniform_(-stdv,stdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, network, optimizer,scheduler, model_path, model_name):\n",
    "        self.network    = network\n",
    "        self.optimizer  = optimizer\n",
    "        self.model_path = model_path\n",
    "        self.model_name = model_name\n",
    "        self.scheduler  = scheduler\n",
    "\n",
    "        self.total_train_loss = []\n",
    "        self.total_val_loss   = []\n",
    "        \n",
    "    def train(self, dataset,n_epochs):\n",
    "        from torch.autograd import Variable\n",
    "        import time\n",
    "\n",
    "        len_dataset         = len(dataset)\n",
    "        n_batches           = int(len(dataset)/btsize + 1)\n",
    "        training_start_time = time.time()\n",
    "\n",
    "        # Sending the data to CPU\n",
    "        dataset.send_device(device) \n",
    "\n",
    "        # --------- Splitting the dataset into training and validation -------\n",
    "        indices            = list(range(int(len_dataset)))\n",
    "        validation_idx     = indices[:int(len_dataset*(validation_per/100))]  #np.random.choice(indices, size=int(len_dataset*(validation_per/100)), replace=False)\n",
    "        train_idx          = list(set(indices) - set(validation_idx))\n",
    "        validation_sampler = SubsetRandomSampler(validation_idx)\n",
    "        train_sampler      = SubsetRandomSampler(train_idx)\n",
    "\n",
    "        train_loader       = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=btsize,\n",
    "            sampler=train_sampler,\n",
    "            )    \n",
    "        validation_loader  = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=btsize,\n",
    "            sampler=validation_sampler,\n",
    "        )    \n",
    "\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            print_every           = 1\n",
    "            start_time            = time.time()\n",
    "            running_sample_count  = 0\n",
    "\n",
    "            total_train_loss      = 0\n",
    "            total_val_loss        = 0\n",
    "\n",
    "\n",
    "            for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "                inputs, labels, indexbatch = data\n",
    "                # Making sure input and labels are floats\n",
    "                inputs.float()\n",
    "                labels.float()\n",
    "                \n",
    "                inputs.requires_grad_()\n",
    "\n",
    "                # Forward pass\n",
    "                pred_labels = self.network(inputs)\n",
    "\n",
    "                loss_ = EikonalLoss(labels,pred_labels)\n",
    "\n",
    "                # Update parameters\n",
    "                loss_.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            # ----- Determining the Training Loss -----\n",
    "            for i, data_train in enumerate(train_loader, 0):\n",
    "                inputs_train, labels_train, indexbatch_train = data_train\n",
    "                inputs_train.requires_grad_()\n",
    "#                 outputs_train,pred_labels_train = self.network(inputs_train)\n",
    "                pred_labels_train = self.network(inputs_train)\n",
    "                train_loss              = EikonalLoss(labels_train,pred_labels_train)\n",
    "                total_train_loss           += train_loss.item()\n",
    "\n",
    "\n",
    "            # ----- Determining the Validation Loss -----\n",
    "            for i, data_val in enumerate(validation_loader, 0):\n",
    "                inputs_val, labels_val, indexbatch_val = data_val\n",
    "                inputs_val.requires_grad_()\n",
    "                pred_labels_val = self.network(inputs_val)\n",
    "#                 outputs_val,pred_labels_val = self.network(inputs_val)\n",
    "                val_loss                 = EikonalLoss(labels_val,pred_labels_val)\n",
    "                total_val_loss             += val_loss.item()\n",
    "#                 del inputs_val, labels_val, indexbatch_val, pred_labels_val, val_loss, wv\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # Creating a running loss for both training and validation data\n",
    "            total_val_loss   /= len(validation_loader)\n",
    "            total_train_loss /= len(train_loader)\n",
    "            self.total_train_loss.append(total_train_loss)\n",
    "            self.total_val_loss.append(total_val_loss)\n",
    "            self.scheduler.step(total_val_loss)\n",
    "\n",
    "#             del train_loader_wei,train_sampler_wei\n",
    "\n",
    "            if (epoch+1) % 10 == 1:\n",
    "                with torch.no_grad():\n",
    "                    print(\"Epoch = {} -- Training loss = {:.4e} -- Validation loss = {:.4e}\".format(epoch+1,total_train_loss,total_val_loss))\n",
    "\n",
    "            if (epoch+1) % 50 == 1:\n",
    "                with torch.no_grad():\n",
    "                    torch.save({\n",
    "                        'epoch':k,\n",
    "                        'model_state_dict': self.network.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'train_loss': self.total_train_loss,\n",
    "                        'val_loss': self.total_val_loss,\n",
    "                        }, '{}/{}_{}_{}.pt'.format(self.model_path, self.model_name,str(epoch).zfill(5),total_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1113900\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# device             = torch.device(\"cuda:{}\".format(GPUid))\n",
    "device = torch.device(\"cpu\")\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "# --------- Defining the neural network -------\n",
    "# torch.cuda.set_device(device)\n",
    "net = Reg_model()\n",
    "net.apply(init_weights)\n",
    "net.float()\n",
    "net.to(device)\n",
    "\n",
    "def count_parameters(net):\n",
    "    return sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(count_parameters(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 596\n",
    "step = 2\n",
    "border = 895\n",
    "\n",
    "Xp_1 = np.load('Data/Xp.npy')\n",
    "Xp_2 = np.load('Data/Xp_1.npy')\n",
    "Xp_initial = np.concatenate((Xp_1,Xp_2))\n",
    "\n",
    "Yp_1 = np.load('Data/Yp.npy')\n",
    "Yp_2 = np.load('Data/Yp_1.npy')\n",
    "Yp_initial = np.concatenate((Yp_1,Yp_2))\n",
    "\n",
    "\n",
    "Data = np.column_stack((Xp_initial,Yp_initial))\n",
    "data_size = len(Data)\n",
    "r = np.random.permutation(data_size)\n",
    "Data = Data[r,:]\n",
    "    \n",
    "Xp_initial = Data[:110000,start:border]\n",
    "Yp_initial = Data[:110000,border::step]\n",
    "    \n",
    "Xp_mean = np.mean(Xp_initial, axis = 0)\n",
    "Xp_std = np.std(Xp_initial, axis = 0)\n",
    "Xp = preprocessing.scale(Xp_initial)\n",
    "\n",
    "Yp_mean = np.mean(Yp_initial, axis = 0)\n",
    "Yp_std = np.std(Yp_initial, axis = 0)\n",
    "Yp = preprocessing.scale(Yp_initial)\n",
    "# Yp = Yp_initial\n",
    "\n",
    "Xp_test = Data[110000:,start:border]\n",
    "Xp_test = (Xp_test - Xp_mean)/Xp_std\n",
    "\n",
    "Yp_test = Data[110000:,border::step]\n",
    "Yp_test = (Yp_test - Yp_mean)/Yp_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2cf878fa047f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./OutputModel/accepted_m_1.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmd\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-8cfc4f5b1387>\u001b[0m in \u001b[0;36mload_checkpoint\u001b[0;34m(filepath, device)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0moptimizer\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_val_loss\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Model' is not defined"
     ]
    }
   ],
   "source": [
    "fname = './OutputModel/accepted_m_1.pt'\n",
    "md    = load_checkpoint(fname,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "plt.plot(range(len(md.total_train_loss)), md.total_train_loss, linewidth=2, label = 'Training loss')\n",
    "plt.plot(range(len(md.total_train_loss)), md.total_val_loss, linewidth=2, label = 'Validation loss')\n",
    "axes = plt.gca()\n",
    "plt.gca().legend(loc=0, prop={'size': 15})\n",
    "axes.set_ylim([0, 0.1])\n",
    "axes.set_xlim([0,150])\n",
    "plt.xticks(fontsize= 10)\n",
    "plt.yticks(fontsize= 10)\n",
    "plt.xlabel('Epochs No.', fontsize = 15, labelpad = 10)\n",
    "plt.ylabel('Loss', fontsize = 15, labelpad = 10)\n",
    "major_ticks = np.arange(0, len(md.total_train_loss), 20)\n",
    "minor_ticks = np.arange(0, len(md.total_train_loss), 5)\n",
    "\n",
    "axes.set_xticks(major_ticks)\n",
    "axes.set_xticks(minor_ticks, minor=True)\n",
    "\n",
    "axes.grid(which='both')\n",
    "axes.grid(color='k', which='minor', alpha=0.2)\n",
    "axes.grid(color='k', which='major', alpha=0.4)\n",
    "# axes.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_start = 200\n",
    "idx_end = 1200\n",
    "idx_len = idx_end - idx_start\n",
    "\n",
    "test_1_x = Xp_test[idx_start:idx_end,:]\n",
    "# test_1_x[110:150] = [None]*(150-110)\n",
    "test_1_x = test_1_x.reshape(idx_len,299)\n",
    "test_1_y = Yp_test[idx_start:idx_end,:]\n",
    "test_1_y = test_1_y.reshape(idx_len,Yp_test.shape[1])\n",
    "test_tensor_x = torch.tensor(test_1_x).float() # added .values when no scaling in preprocess\n",
    "test_tensor_y = torch.tensor(test_1_y).float() # added .values when no scaling in preprocess\n",
    "\n",
    "\n",
    "bs = Yp_test.shape[1]\n",
    "\n",
    "test_loaded = torch.utils.data.TensorDataset(test_tensor_x,test_tensor_y)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_loaded,batch_size=bs, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_size_test, (data, target) in enumerate(test_loader):\n",
    "        data, target = Variable(data).to(device), Variable(target).to(device)\n",
    "        output = md.network(data)\n",
    "        break\n",
    "        \n",
    "\n",
    "prediction = output.numpy()*Yp_std + Yp_mean\n",
    "targettt = target.numpy()*Yp_std + Yp_mean\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(np.array([0, 4000]), np.array([0, 4000]), 'k--',  linewidth=1)\n",
    "plt.plot(prediction, targettt, 'o', linewidth=2)\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0, 4000])\n",
    "axes.set_xlim([0,4000])\n",
    "plt.xticks(fontsize= 10)\n",
    "plt.yticks(fontsize= 10)\n",
    "plt.xlabel('Target values', fontsize = 15, labelpad = 10)\n",
    "plt.ylabel('Predicted values', fontsize = 15, labelpad = 10)\n",
    "# plt.grid(color='k', which = 'both', linestyle='--', linewidth=0.5)\n",
    "major_ticks = np.arange(0,4000, 400)\n",
    "minor_ticks = np.arange(0,4000, 200)\n",
    "r2_val = r2_score(prediction[0,:],targettt[0,:])\n",
    "plt.text(500,2500,r'$r^2:$'+ '{:.4f}'.format(r2_val), fontsize=15)\n",
    "axes.set_xticks(major_ticks)\n",
    "axes.set_xticks(minor_ticks, minor=True)\n",
    "\n",
    "axes.grid(which='both')\n",
    "axes.grid(color='k', which='minor', alpha=0.2)\n",
    "axes.grid(color='k', which='major', alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idx = 100\n",
    "\n",
    "test_1_x = Xp_test[idx,:]\n",
    "test_1_x = test_1_x.reshape(1,299)\n",
    "test_1_y = Yp_test[idx,:]\n",
    "test_1_y = test_1_y.reshape(1,Yp_test.shape[1])\n",
    "test_tensor_x = torch.tensor(test_1_x).float() # added .values when no scaling in preprocess\n",
    "test_tensor_y = torch.tensor(test_1_y).float() # added .values when no scaling in preprocess\n",
    "\n",
    "\n",
    "bs = Yp_test.shape[1]\n",
    "\n",
    "test_loaded = torch.utils.data.TensorDataset(test_tensor_x,test_tensor_y)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_loaded,batch_size=bs, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_size_test, (data, target) in enumerate(test_loader):\n",
    "        data, target = Variable(data).to(device), Variable(target).to(device)\n",
    "        output = md.network(data)\n",
    "        break\n",
    "        \n",
    "\n",
    "prediction = output.numpy()*Yp_std + Yp_mean\n",
    "targettt = target.numpy()*Yp_std + Yp_mean\n",
    "\n",
    "depth = -np.arange(0,500,4)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "plt.step(prediction.reshape(Yp_test.shape[1],), depth, linewidth=2, label = 'Prediction')\n",
    "plt.step(targettt.reshape(Yp_test.shape[1],), depth, linewidth=2, label = 'Target')\n",
    "axes = plt.gca()\n",
    "plt.gca().legend(loc=0, prop={'size': 15})\n",
    "axes.set_ylim([-400, 0])\n",
    "plt.xticks(fontsize= 10)\n",
    "plt.yticks(fontsize= 10)\n",
    "plt.xlabel('Shear Wave Velocity (m/s)', fontsize = 15, labelpad = 10)\n",
    "plt.ylabel('Depth', fontsize = 15, labelpad = 10)\n",
    "r2_val = r2_score(prediction[0,:],targettt[0,:])\n",
    "plt.text(200,-300,r'$r^2:$'+ '{:.4f}'.format(r2_val), fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 700\n",
    "\n",
    "test_1_x = Xp_test[idx,:]\n",
    "# test_1_x[110:150] = [None]*(150-110)\n",
    "test_1_x = test_1_x.reshape(1,299)\n",
    "test_1_y = Yp_test[idx,:]\n",
    "test_1_y = test_1_y.reshape(1,Yp_test.shape[1])\n",
    "test_tensor_x = torch.tensor(test_1_x).float() # added .values when no scaling in preprocess\n",
    "test_tensor_y = torch.tensor(test_1_y).float() # added .values when no scaling in preprocess\n",
    "\n",
    "\n",
    "bs = Yp_test.shape[1]\n",
    "\n",
    "test_loaded = torch.utils.data.TensorDataset(test_tensor_x,test_tensor_y)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_loaded,batch_size=bs, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_size_test, (data, target) in enumerate(test_loader):\n",
    "        data, target = Variable(data).to(device), Variable(target).to(device)\n",
    "        output = md.network(data)\n",
    "        break\n",
    "        \n",
    "\n",
    "prediction = output.numpy()*Yp_std + Yp_mean\n",
    "targettt = target.numpy()*Yp_std + Yp_mean\n",
    "\n",
    "depth = -np.arange(0,500,4)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "plt.step(prediction.reshape(Yp_test.shape[1],), depth, linewidth=2, label = 'Prediction')\n",
    "plt.step(targettt.reshape(Yp_test.shape[1],), depth, linewidth=2, label = 'Target')\n",
    "axes = plt.gca()\n",
    "plt.gca().legend(loc=0, prop={'size': 15})\n",
    "axes.set_ylim([-400, 0])\n",
    "plt.xticks(fontsize= 10)\n",
    "plt.yticks(fontsize= 10)\n",
    "plt.xlabel('Shear Wave Velocity (m/s)', fontsize = 15, labelpad = 10)\n",
    "plt.ylabel('Depth', fontsize = 15, labelpad = 10)\n",
    "r2_val = r2_score(prediction[0,:],targettt[0,:])\n",
    "plt.text(200,-300,r'$r^2:$'+ '{:.4f}'.format(r2_val), fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 500\n",
    "\n",
    "test_1_x = Xp_test[idx,:]\n",
    "test_1_x = test_1_x.reshape(1,299)\n",
    "test_1_y = Yp_test[idx,:]\n",
    "test_1_y = test_1_y.reshape(1,Yp_test.shape[1])\n",
    "test_tensor_x = torch.tensor(test_1_x).float() # added .values when no scaling in preprocess\n",
    "test_tensor_y = torch.tensor(test_1_y).float() # added .values when no scaling in preprocess\n",
    "\n",
    "\n",
    "bs = Yp_test.shape[1]\n",
    "\n",
    "test_loaded = torch.utils.data.TensorDataset(test_tensor_x,test_tensor_y)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_loaded,batch_size=bs, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_size_test, (data, target) in enumerate(test_loader):\n",
    "        data, target = Variable(data).to(device), Variable(target).to(device)\n",
    "        output = md.network(data)\n",
    "        break\n",
    "        \n",
    "\n",
    "prediction = output.numpy()*Yp_std + Yp_mean\n",
    "targettt = target.numpy()*Yp_std + Yp_mean\n",
    "\n",
    "# prediction = output.numpy()\n",
    "# targettt = target.numpy()\n",
    "\n",
    "depth = -np.arange(0,500,4)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "plt.step(prediction.reshape(Yp_test.shape[1],), depth, linewidth=2, label = 'Prediction')\n",
    "plt.step(targettt.reshape(Yp_test.shape[1],), depth, linewidth=2, label = 'Target')\n",
    "axes = plt.gca()\n",
    "plt.gca().legend(loc=0, prop={'size': 15})\n",
    "axes.set_ylim([-400, 0])\n",
    "plt.xticks(fontsize= 10)\n",
    "plt.yticks(fontsize= 10)\n",
    "plt.xlabel('Shear Wave Velocity (m/s)', fontsize = 15, labelpad = 10)\n",
    "plt.ylabel('Depth', fontsize = 15, labelpad = 10)\n",
    "r2_val = r2_score(prediction[0,:],targettt[0,:])\n",
    "plt.text(200,-300,r'$r^2:$'+ '{:.4f}'.format(r2_val), fontsize=15)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
