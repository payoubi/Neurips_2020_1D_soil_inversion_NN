{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pylab as plt\n",
    "import random\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch import Tensor\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import SGD, Adam, RMSprop\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.utils.data.sampler import SubsetRandomSampler,WeightedRandomSampler\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "manualSeed = 1 # random.randint(1, 10000) # Set the desired seed to reproduce the results\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset includes 5 input values (aspect ratio, dimensionless frequency, velocity contrast, frequency component), and real and imaginary part of time-series as output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateDataset(PATH):\n",
    "    \n",
    "    Xp_1 = np.load('Data/Xp.npy')\n",
    "    Xp_2 = np.load('Data/Xp_1.npy')\n",
    "    Xp = np.concatenate((Xp_1,Xp_2))\n",
    "    \n",
    "    Yp_1 = np.load('Data/Yp.npy')\n",
    "    Yp_2 = np.load('Data/Yp_1.npy')\n",
    "    Yp = np.concatenate((Yp_1,Yp_2))\n",
    "    \n",
    "    Data = np.column_stack((Xp,Yp))\n",
    "    data_size = len(Data)\n",
    "    r = np.random.permutation(data_size)\n",
    "    Data = Data[r,:]\n",
    "    start = 596\n",
    "    step = 2\n",
    "    border = 895\n",
    "    \n",
    "    \n",
    "    Xp = Data[:110000,start:895]\n",
    "    Yp = Data[:110000,895::step]\n",
    "    \n",
    "    \n",
    "    Xp = preprocessing.scale(Xp)\n",
    "    \n",
    "    Yp = preprocessing.scale(Yp)\n",
    "    \n",
    "    Xp_test = Data[110000:,start:895]\n",
    "    Yp_test = Data[110000:,895::step]\n",
    "    \n",
    "    \n",
    "    return Xp,Yp, Xp_test, Yp_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Achietecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reg_model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Defining the neural network\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        m = 1\n",
    "        # Layers \n",
    "        self.fc0  = Linear(299,299*m)\n",
    "        self.fc1  = Linear(299*m,299*m)\n",
    "\n",
    "        # resnet - block 1\n",
    "        self.rn1_fc1  = Linear(299*m,299*m)\n",
    "        self.rn1_fc2  = Linear(299*m,299*m)\n",
    "        self.rn1_fc3  = Linear(299*m,299*m)\n",
    "\n",
    "        # resnet - block 2 \n",
    "        self.rn2_fc1  = Linear(299*m,299*m)\n",
    "        self.rn2_fc2  = Linear(299*m,299*m)\n",
    "        self.rn2_fc3  = Linear(299*m,299*m)\n",
    "\n",
    "        # resnet - block 3\n",
    "        self.rn3_fc1  = Linear(299*m,299*m)\n",
    "        self.rn3_fc2  = Linear(299*m,299*m)\n",
    "        self.rn3_fc3  = Linear(299*m,299*m)\n",
    "\n",
    "\n",
    "        # Output structure\n",
    "        self.fc8  = Linear(299*m,299)\n",
    "        self.fc9  = Linear(299,125)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x   = self.relu(self.fc0(x))\n",
    "        x   = self.relu(self.fc1(x))\n",
    "\n",
    "        # Resnet - Block 1\n",
    "        x0  = x\n",
    "        x   = self.relu(self.rn1_fc1(x))\n",
    "        x   = self.relu(self.rn1_fc3(x) + self.rn1_fc2(x0))\n",
    "\n",
    "\n",
    "        # Resnet - Block 2\n",
    "        x0  = x\n",
    "        x   = self.relu(self.rn2_fc1(x))\n",
    "        x   = self.relu(self.rn2_fc3(x)+self.rn2_fc2(x0))\n",
    "\n",
    "#         # Resnet - Block 3\n",
    "        x0  = x\n",
    "        x   = self.relu(self.rn3_fc1(x))\n",
    "        x   = self.relu(self.rn3_fc3(x)+self.rn3_fc2(x0))\n",
    "\n",
    "\n",
    "\n",
    "        # Joining two blocks\n",
    "        x     = self.relu(self.fc8(x))\n",
    "        x   = self.fc9(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data, target, transform=None):\n",
    "        # Creating identical pairs\n",
    "        self.data    = Variable(Tensor(data))\n",
    "        self.target  = Variable(Tensor(target))\n",
    "\n",
    "    def send_device(self,device):\n",
    "        self.data    = self.data.to(device)\n",
    "        self.target  = self.target.to(device)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "        return x, y, index\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == torch.nn.Linear:\n",
    "        stdv = (1. / math.sqrt(m.weight.size(1))/1.)*2\n",
    "        m.weight.data.uniform_(-stdv,stdv)\n",
    "        m.bias.data.uniform_(-stdv,stdv)\n",
    "\n",
    "def Loss_fun(Yobs,Ypred):\n",
    "\n",
    "        loss_2 = loss_funcl2(Ypred, Yobs)\n",
    "        loss_1 = loss_funcl1(Ypred, Yobs)/10\n",
    "    \n",
    "#         return loss_1+loss_2\n",
    "        return loss_2\n",
    "\n",
    "# def eval(model, val_loader, device):\n",
    "#     model.eval()\n",
    "#     l2 = 0\n",
    "#     l1 = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for batch_idx, (data, target) in enumerate(val_loader):\n",
    "#             data, target = Variable(data).to(device), Variable(target).to(device)\n",
    "#             output = model(data)\n",
    "#             l2 += loss_funcl2(output, target)\n",
    "#             l1 += loss_funcl1(output, target)\n",
    "            \n",
    "            \n",
    "            \n",
    "#     l1 /= len(val_loader.dataset) / bs\n",
    "#     l2 /= len(val_loader.dataset) / bs\n",
    "    \n",
    "# #     model.train()\n",
    "#     return l2.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, network, optimizer,scheduler, model_path, model_name):\n",
    "        self.network    = network\n",
    "        self.optimizer  = optimizer\n",
    "        self.model_path = model_path\n",
    "        self.model_name = model_name\n",
    "        self.scheduler  = scheduler\n",
    "\n",
    "        self.total_train_loss = []\n",
    "        self.total_val_loss   = []\n",
    "        \n",
    "    def train(self, dataset,n_epochs):\n",
    "        from torch.autograd import Variable\n",
    "        import time\n",
    "\n",
    "        len_dataset         = len(dataset)\n",
    "        n_batches           = int(len(dataset)/btsize + 1)\n",
    "        training_start_time = time.time()\n",
    "\n",
    "        # Sending the data to CPU\n",
    "        dataset.send_device(device) \n",
    "\n",
    "        # --------- Splitting the dataset into training and validation -------\n",
    "        indices            = list(range(int(len_dataset)))\n",
    "        validation_idx     = indices[:int(len_dataset*(validation_per/100))]  #np.random.choice(indices, size=int(len_dataset*(validation_per/100)), replace=False)\n",
    "        train_idx          = list(set(indices) - set(validation_idx))\n",
    "        validation_sampler = SubsetRandomSampler(validation_idx)\n",
    "        train_sampler      = SubsetRandomSampler(train_idx)\n",
    "\n",
    "        train_loader       = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=btsize,\n",
    "            sampler=train_sampler,\n",
    "            )    \n",
    "        validation_loader  = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=btsize,\n",
    "            sampler=validation_sampler,\n",
    "        )    \n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            print_every           = 1\n",
    "            start_time            = time.time()\n",
    "            running_sample_count  = 0\n",
    "\n",
    "            total_train_loss      = 0\n",
    "            total_val_loss        = 0\n",
    "\n",
    "\n",
    "            for i, data in enumerate(train_loader, 0):\n",
    "                #print('----------------- Epoch {} - Batch {} --------------------'.format(epoch,i))\n",
    "                \n",
    "                # Get inputs/outputs and wrap in variable object\n",
    "                inputs, labels, indexbatch = data\n",
    "                # Making sure input and labels are floats\n",
    "                inputs.float()\n",
    "                labels.float()\n",
    "                \n",
    "                inputs.requires_grad_()\n",
    "\n",
    "                # Forward pass\n",
    "                pred_labels = self.network(inputs)\n",
    "                loss_ = Loss_fun(labels,pred_labels)\n",
    "\n",
    "    \n",
    "                # Update parameters\n",
    "                loss_.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "\n",
    "\n",
    "            # ----- Determining the Training Loss -----\n",
    "            for i, data_train in enumerate(train_loader, 0):\n",
    "                inputs_train, labels_train, indexbatch_train = data_train\n",
    "                inputs_train.requires_grad_()\n",
    "#                 outputs_train,pred_labels_train = self.network(inputs_train)\n",
    "                pred_labels_train = self.network(inputs_train)\n",
    "                train_loss              = Loss_fun(labels_train,pred_labels_train)\n",
    "                total_train_loss           += train_loss.item()\n",
    "#                 del inputs_train, labels_train, indexbatch_train, pred_labels_train, train_loss, wv\n",
    "\n",
    "\n",
    "            # ----- Determining the Validation Loss -----\n",
    "            for i, data_val in enumerate(validation_loader, 0):\n",
    "                inputs_val, labels_val, indexbatch_val = data_val\n",
    "                inputs_val.requires_grad_()\n",
    "                pred_labels_val = self.network(inputs_val)\n",
    "                val_loss                 = Loss_fun(labels_val,pred_labels_val)\n",
    "                total_val_loss             += val_loss.item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # Creating a running loss for both training and validation data\n",
    "            total_val_loss   /= len(validation_loader)\n",
    "            total_train_loss /= len(train_loader)\n",
    "            self.total_train_loss.append(total_train_loss)\n",
    "            self.total_val_loss.append(total_val_loss)\n",
    "            self.scheduler.step(total_val_loss)\n",
    "\n",
    "            if (epoch+1) % 10 == 1:\n",
    "                with torch.no_grad():\n",
    "                    print(\"Epoch = {} -- Training loss = {:.4e} -- Validation loss = {:.4e}\".format(epoch+1,total_train_loss,total_val_loss))\n",
    "\n",
    "            if (epoch+1) % 100 == 1:\n",
    "                with torch.no_grad():\n",
    "                    torch.save({\n",
    "                        'epoch':epoch,\n",
    "                        'model_state_dict': self.network.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'train_loss': self.total_train_loss,\n",
    "                        'val_loss': self.total_val_loss,\n",
    "                        }, '{}/{}_{}_{}.pt'.format(self.model_path, self.model_name,str(epoch).zfill(5),total_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110000, 299)\n",
      "(110000, 125)\n",
      "1113900\n"
     ]
    }
   ],
   "source": [
    "num_epochs         = 5000\n",
    "validation_per     = 10\n",
    "btsize             = 256\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "PATH                                 = './OutputModels'\n",
    "Xp,Yp, Xp_test, Yp_test              = CreateDataset(PATH)\n",
    "dataset                              = NumpyDataset(Xp,Yp)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# device             = torch.device(\"cuda:{}\".format(GPUid))\n",
    "device = torch.device(\"cpu\")\n",
    "torch.set_num_threads(10)\n",
    "\n",
    "# --------- Defining the neural network -------\n",
    "net = Reg_model()\n",
    "net.apply(init_weights)\n",
    "net.float()\n",
    "net.to(device)\n",
    "loss_funcl2 = torch.nn.MSELoss(reduction='mean')\n",
    "loss_funcl1 = torch.nn.L1Loss(reduction='mean')\n",
    "\n",
    "def count_parameters(net):\n",
    "    return sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(count_parameters(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## --------- Defining optimizer -------\n",
    "optimizer  = torch.optim.Adam(net.parameters(),lr=1e-4)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "# --------- Training Model -------\n",
    "model = Model(net, optimizer,scheduler,model_path=PATH,model_name='1D_inversion')\n",
    "model.train(dataset,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
